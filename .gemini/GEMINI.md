Gemini CLI Document Worker PRD
1. Project Goal
The primary goal of this project is to develop and deploy a Cloudflare Worker that acts as an intelligent document repository. This worker will receive documents, process them for OCR text and embeddings, store them in a durable, globally distributed fashion, and expose a flexible API for both human users and AI models to interact with the content. This will enable a one-off "ask my PDF" functionality by simply sharing a unique URL, and also lay the foundation for more advanced retrieval-augmented generation (RAG) workflows.

2. Target Audience & Stakeholders
End-Users: Individuals who want to quickly process a document and share it with an AI model for ad-hoc Q&A.

Gemini CLI Developers: The team responsible for building the gemini cli script that will post processed documents to the worker.

AI Models: The various agents (e.g., via OpenAI, Gemini, Anthropic) that will receive the unique file URLs and interact with the worker's API.

3. User Stories
As a gemini cli user, I want to OCR a document with a local Python script and post it to the worker so that it is indexed and ready for retrieval.

As a user, I want to post a file directly to the worker and receive a unique URL so I can immediately use it in an AI chat.

As a developer of an AI agent, I want a unique URL to retrieve a document's content, its embeddings, or to query it directly via a simple HTTP request.

As a human user browsing the web, I want to be able to navigate to a document's URL and view its extracted plain text, or access a web frontend to perform a chat query against its content.

4. Functional Requirements
4.1. Document Ingestion
The worker must handle two primary methods of document ingestion:

API Upload (POST): A POST request to the worker's root URL / with the file content.

The worker will generate a unique fileId.

The worker will store the raw file in an R2 bucket.

The worker will run OCR (via a gemini-cli hook or Worker AI if possible) to extract text.

The worker will generate embeddings from the extracted text using Worker AI.

The worker will store the fileId, R2 pointer, extracted text, and embeddings in D1 and/or Vectorize.

The response will be a JSON object containing the unique URL (https://your-worker.hacolby.workers.dev/<fileId>).

4.2. Public-Facing Endpoints
Each document will have a unique fileId that forms the base of its URL (https://your-worker.hacolby.workers.dev/<fileId>). The following endpoints are required:

Endpoint

Method

Description

/<fileId>

GET

Renders the plain extracted text of the document in a web browser.

/<fileId>/embeddings

GET

Renders the JSON representation of the document's embeddings in a web browser.

/<fileId>/ask

GET

Serves a simple HTML chat frontend in the browser. Users can type questions about the document and receive answers via a POST request to the same endpoint.

/<fileId>/ask

POST

Accepts a JSON payload { "query": "Your question here" } and returns a generated answer about the document using Worker AI.

/<fileId>/semantic

GET

Serves a simple HTML chat frontend in the browser, similar to /ask, but specifically for semantic search.

/<fileId>/semantic

POST

Accepts a JSON payload { "query": "Your search query" } and returns the most semantically similar chunks of text from the document.

5. Technical Specifications
Cloudflare Worker: The central point of logic, handling all routing, requests, and integration with other services.

Cloudflare R2: Used for durable, egress-free storage of the original, unprocessed documents. This acts as the "reading and writing interface."

Cloudflare D1: Used to store document metadata, including the fileId, original filename, R2 bucket path, and extracted text. This will act as the primary, highly-available index.

Cloudflare Worker AI:

Text Extraction: Used for OCR if a file is posted directly.

Embeddings Generation: The embedding model will be used to generate vector representations of document chunks.

Generative AI: The LLM will be used to answer questions posed to the /ask endpoint, leveraging the document content.

Cloudflare Vectorize: The vector database to store the embeddings generated by Worker AI. It will be the engine behind the /semantic search endpoint.

6. Future Considerations
Authentication & Access Control: Implement token-based authentication to restrict who can post and access documents.

Multi-File RAG: Expand the /ask and /semantic endpoints to query across multiple documents or an entire user-specific repository, rather than just one file.

Deletion Policy: Implement a mechanism for document deletion, either on a user request or with an automatic TTL (Time-to-Live).

Progressive Web App (PWA): Develop the chat frontends as a PWA for a more native-like user experience.
